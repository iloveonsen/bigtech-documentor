{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "134ed57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라이브러리 임포트 완료\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 임포트\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PDF 처리\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "# DeepSeek-OCR\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# RAG 시스템\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "import faiss\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# 환경 변수\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "import tempfile, shutil\n",
    "\n",
    "tmpdir = tempfile.mkdtemp(prefix=\"deepseekocr_\")\n",
    "\n",
    "# API 키 로드\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "print(\"라이브러리 임포트 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82c13859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 폴더 생성\n",
    "ORIGIN_DIR = Path(\"./origin\")      # PDF 논문 폴더\n",
    "IMAGES_DIR = Path(\"./images\")      # PDF → 이미지 결과\n",
    "EXTRACTED_DIR = Path(\"./extracted\") # 추출된 이미지 저장 폴더\n",
    "# FIGURES_DIR = Path(\"./figures\")    # 추출된 figure/table\n",
    "DATA_DIR = Path(\"./data\")          # 메타데이터 및 description 저장\n",
    "\n",
    "for directory in [ORIGIN_DIR, IMAGES_DIR, EXTRACTED_DIR, DATA_DIR]:\n",
    "    if not directory.exists():\n",
    "        directory.mkdir(exist_ok=True)\n",
    "        print(f\"폴더 생성: {directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68448351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_high_res_images(pdf_path: str, output_dir: Path, dpi: int = 300) -> List[Path]:\n",
    "    \"\"\"\n",
    "    PDF를 고해상도 이미지로 변환\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: PDF 파일 경로\n",
    "        output_dir: 출력 폴더\n",
    "        dpi: 해상도 (기본: 300, 고해상도)\n",
    "    \n",
    "    Returns:\n",
    "        저장된 이미지 경로 리스트\n",
    "    \"\"\"\n",
    "    print(f\"\\nPDF 변환 중: {pdf_path}\")\n",
    "    print(f\"해상도: {dpi} DPI\")\n",
    "    \n",
    "    # PDF 이름 추출\n",
    "    pdf_name = Path(pdf_path).stem\n",
    "    \n",
    "    # 이미지로 변환\n",
    "    images = convert_from_path(pdf_path, dpi=dpi)\n",
    "    \n",
    "    # 저장\n",
    "    image_paths = []\n",
    "    for i, image in enumerate(images, 1):\n",
    "        image_path = output_dir / f\"{pdf_name}_page_{i:03d}.png\"\n",
    "        image.save(image_path, \"PNG\")\n",
    "        image_paths.append(image_path)\n",
    "        print(f\"페이지 {i}/{len(images)} 저장: {image_path.name}\")\n",
    "    \n",
    "    print(f\"\\n변환 완료: {len(images)}개 페이지\")\n",
    "    return image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50cd84e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "발견된 PDF 파일: 1개\n",
      "  - LegalRAG-A_Hybrid_RAG_System_for_Multilingual_Legal_Information_Retrieval.pdf\n",
      "\n",
      "PDF 변환 중: origin/LegalRAG-A_Hybrid_RAG_System_for_Multilingual_Legal_Information_Retrieval.pdf\n",
      "해상도: 300 DPI\n",
      "페이지 1/8 저장: LegalRAG-A_Hybrid_RAG_System_for_Multilingual_Legal_Information_Retrieval_page_001.png\n",
      "페이지 2/8 저장: LegalRAG-A_Hybrid_RAG_System_for_Multilingual_Legal_Information_Retrieval_page_002.png\n",
      "페이지 3/8 저장: LegalRAG-A_Hybrid_RAG_System_for_Multilingual_Legal_Information_Retrieval_page_003.png\n",
      "페이지 4/8 저장: LegalRAG-A_Hybrid_RAG_System_for_Multilingual_Legal_Information_Retrieval_page_004.png\n",
      "페이지 5/8 저장: LegalRAG-A_Hybrid_RAG_System_for_Multilingual_Legal_Information_Retrieval_page_005.png\n",
      "페이지 6/8 저장: LegalRAG-A_Hybrid_RAG_System_for_Multilingual_Legal_Information_Retrieval_page_006.png\n",
      "페이지 7/8 저장: LegalRAG-A_Hybrid_RAG_System_for_Multilingual_Legal_Information_Retrieval_page_007.png\n",
      "페이지 8/8 저장: LegalRAG-A_Hybrid_RAG_System_for_Multilingual_Legal_Information_Retrieval_page_008.png\n",
      "\n",
      "변환 완료: 8개 페이지\n"
     ]
    }
   ],
   "source": [
    "# PDF 파일 리스트 가져오기\n",
    "pdf_files = list(ORIGIN_DIR.glob(\"*.pdf\"))\n",
    "print(f\"발견된 PDF 파일: {len(pdf_files)}개\")\n",
    "for pdf_file in pdf_files:\n",
    "    print(f\"  - {pdf_file.name}\")\n",
    "\n",
    "# 모든 PDF를 이미지로 변환\n",
    "all_page_images = {}\n",
    "for pdf_file in pdf_files:\n",
    "    image_paths = pdf_to_high_res_images(pdf_file, IMAGES_DIR, dpi=300)\n",
    "    all_page_images[pdf_file.stem] = image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e1f2d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0acc29cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DeepSeek-OCR 모델 로딩: deepseek-ai/DeepSeek-OCR\n"
     ]
    }
   ],
   "source": [
    "model_name = 'deepseek-ai/DeepSeek-OCR'\n",
    "print(f\"\\nDeepSeek-OCR 모델 로딩: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79544341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "627971432878407ea9216eadcbcc07aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af979822f9e0485cbbc4b65d0ac6d299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4cebbcce54b46bb9fae105cb670ab06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/801 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e702b0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flash-attn 사용\n",
      "GPU: NVIDIA GeForce RTX 3090\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c4e8720acad4ab791cfc6ea778a213e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d0f968f6f244b778a9f7b8fe0a2a62e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_deepseekocr.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-OCR:\n",
      "- modeling_deepseekocr.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f091f40208794d08b7a663d46fc4d965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daff3dcf792b49219d3d279efc3f247e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c124b8922554664a9e1233d7bf92d73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-000001.safetensors:   0%|          | 0.00/6.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at deepseek-ai/DeepSeek-OCR and are newly initialized: ['model.vision_model.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 로딩 완료\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# flash-attn 체크\n",
    "try:\n",
    "    import flash_attn\n",
    "    attn_impl = \"flash_attention_2\"\n",
    "    print(\"flash-attn 사용\")\n",
    "except ImportError:\n",
    "    attn_impl = \"eager\"\n",
    "    print(\"기본 attention 사용\")\n",
    "\n",
    "# 모델 로드\n",
    "model_kwargs = {\n",
    "    \"trust_remote_code\": True,\n",
    "    \"attn_implementation\": attn_impl\n",
    "}\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    model_kwargs[\"torch_dtype\"] = torch.bfloat16\n",
    "    # 단일 GPU만 사용하도록 명시적으로 지정\n",
    "    model_kwargs[\"device_map\"] = \"cuda:0\"\n",
    "else:\n",
    "    print(\"CPU 모드 (느림)\")\n",
    "    model_kwargs[\"device_map\"] = \"cpu\"\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name, **model_kwargs)\n",
    "model = model.eval()\n",
    "print(\"모델 로딩 완료\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f4646e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_figures_with_grounding(tokenizer, model, image_path: Path) -> Dict:\n",
    "    \"\"\"\n",
    "    이미지에서 figure/table 위치 추출\n",
    "    \n",
    "    Returns:\n",
    "        {\n",
    "            'text': 전체 OCR 텍스트,\n",
    "            'figures': [{'name': 'Figure 1', 'caption': '...', 'bbox': [x1,y1,x2,y2]}, ...]\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Grounding 프롬프트로 객체 위치 감지\n",
    "    prompt = \"<image>\\n<|grounding|>Identify and locate all figures, tables, and charts in this page. For each, provide the bounding box coordinates and the associated caption.\"\n",
    "    print(f\"Inside locate_figures_with_grounding - Processing image: {str(image_path)}\")\n",
    "    \n",
    "    try:\n",
    "        # output_path 전달, save_results=False로 설정하여 반환값 받기 시도\n",
    "        result = model.infer(\n",
    "            tokenizer,\n",
    "            prompt=prompt,\n",
    "            image_file=str(image_path),\n",
    "            output_path=tmpdir,\n",
    "            base_size=1024,\n",
    "            image_size=640,\n",
    "            crop_mode=True,\n",
    "            eval_mode=True,\n",
    "            save_results=False  # 파일 저장 대신 반환값 사용\n",
    "        )\n",
    "        \n",
    "        # None 체크\n",
    "        if result is None:\n",
    "            print(f\"경고: infer가 None 반환 - 빈 결과 반환\")\n",
    "            return \"\"\n",
    "        \n",
    "        return result.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"추출 요류: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "046094bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def free_ocr(tokenizer, model, image_path: Path) -> str:\n",
    "    \"\"\"\n",
    "    이미지에서 전체 텍스트 추출 (OCR)\n",
    "    \n",
    "    Returns:\n",
    "        전체 OCR 텍스트\n",
    "    \"\"\"\n",
    "    prompt = \"<image>\\nFree OCR.\"\n",
    "    \n",
    "    try:\n",
    "        result = model.infer(\n",
    "            tokenizer,\n",
    "            prompt=prompt,\n",
    "            image_file=str(image_path),\n",
    "            output_path=tmpdir,\n",
    "            base_size=1024,\n",
    "            image_size=640,\n",
    "            crop_mode=True,\n",
    "            eval_mode=True,\n",
    "            save_results=False  # 파일 저장 대신 반환값 사용\n",
    "        )\n",
    "        \n",
    "        # None 체크\n",
    "        if result is None:\n",
    "            print(f\"경고: infer가 None 반환 - 빈 결과 반환\")\n",
    "            return \"\"\n",
    "        \n",
    "        return result.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"OCR 오류: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "654e9bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_html_quotes(html: str) -> str:\n",
    "    \"\"\"\n",
    "    HTML 속성 값을 작은따옴표(')로 통일\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    # prettify는 자동으로 인덴트 넣기 때문에 formatter=\"minimal\"로 최소한의 포맷만\n",
    "    return soup.decode(formatter=\"minimal\").replace('\"', \"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcb0fb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_table(tokenizer, model, image_path: Path) -> str:\n",
    "    \"\"\"\n",
    "    이미지에서 전체 텍스트 추출 (OCR)\n",
    "    \n",
    "    Returns:\n",
    "        전체 OCR 텍스트\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = \"<image>\\n<|grounding|>Convert the document to markdown.\"\n",
    "    \n",
    "    try:\n",
    "        result = model.infer(\n",
    "            tokenizer,\n",
    "            prompt=prompt,\n",
    "            image_file=str(image_path),\n",
    "            output_path=tmpdir,\n",
    "            base_size=1024,\n",
    "            image_size=640,\n",
    "            crop_mode=True,\n",
    "            eval_mode=True,\n",
    "            save_results=False  # 파일 저장 대신 반환값 사용\n",
    "        )\n",
    "        result = result.strip()\n",
    "\n",
    "        tag = re.search(r'<table>.*?</table>', result, re.DOTALL)\n",
    "        result = tag.group(0) if tag else None\n",
    "        \n",
    "        # None 체크\n",
    "        if result is None:\n",
    "            print(f\"경고: infer가 None 반환 - 빈 결과 반환\")\n",
    "            return \"\"\n",
    "        \n",
    "        # if result.startswith(\"Do\"):\n",
    "        #     result = re.sub(r'^[Dd]o not.*?<table>', '<table>', result, count=1, flags=re.S)\n",
    "\n",
    "        result = normalize_html_quotes(result)\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"OCR 오류: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0560db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_detailed_description(tokenizer, model, image_path: Path) -> str:\n",
    "    \"\"\"\n",
    "    이미지에 대한 상세한 설명 생성\n",
    "    \n",
    "    Args:\n",
    "        image_path: 이미지 파일 경로\n",
    "    \n",
    "    Returns:\n",
    "        상세 설명 텍스트 (plain text, LLM 처리에 최적화)\n",
    "    \"\"\"\n",
    "    prompt = \"<image>\\nDescribe this image in detail.\"\n",
    "    \n",
    "    try:\n",
    "        result = model.infer(\n",
    "            tokenizer,\n",
    "            prompt=prompt,\n",
    "            image_file=str(image_path),\n",
    "            output_path=tmpdir,\n",
    "            base_size=1024,\n",
    "            image_size=640,\n",
    "            crop_mode=True,\n",
    "            eval_mode=True,\n",
    "            save_results=False\n",
    "        )\n",
    "        \n",
    "        if result is None:\n",
    "            print(f\"경고: infer가 None 반환 - 빈 결과 반환\")\n",
    "            return \"\"\n",
    "        \n",
    "        # Grounding 태그 제거 (있을 경우)\n",
    "        result = re.sub(r'<\\|[^|]+\\|>', '', result)\n",
    "        result = re.sub(r'<\\|\\/[^|]+\\|>', '', result)\n",
    "        \n",
    "        return result.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"생성 오류: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c1b029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_save_figure(image_path: Path, bbox: List[float], save_path: Path) -> Path:\n",
    "    \"\"\"\n",
    "    이미지에서 bounding box 영역을 crop하여 저장\n",
    "    \n",
    "    Args:\n",
    "        image_path: 원본 이미지 경로\n",
    "        bbox: [x1, y1, x2, y2] 좌표 (비율 0-1 또는 픽셀 좌표)\n",
    "        output_path: 저장할 경로\n",
    "    \n",
    "    Returns:\n",
    "        저장된 이미지 경로\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    width, height = image.size\n",
    "    \n",
    "    # 좌표가 0-1 비율인 경우 픽셀로 변환\n",
    "    if all(0 <= coord <= 1 for coord in bbox):\n",
    "        x1 = int(bbox[0] * width)\n",
    "        y1 = int(bbox[1] * height)\n",
    "        x2 = int(bbox[2] * width)\n",
    "        y2 = int(bbox[3] * height)\n",
    "    else:\n",
    "        x1, y1, x2, y2 = [int(c) for c in bbox]\n",
    "    \n",
    "    # Crop\n",
    "    cropped = image.crop((x1, y1, x2, y2))\n",
    "    \n",
    "    # 저장\n",
    "    cropped.save(save_path, \"PNG\")\n",
    "    print(f\"    - 크롭 저장 완료: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd10c1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_grounding_result(text: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Grounding 결과 파싱\n",
    "    DeepSeek-OCR의 실제 출력 형식에 맞게 파싱\n",
    "    \n",
    "    실제 형식:\n",
    "    <|ref|>image<|/ref|><|det|>[[x1, y1, x2, y2]]<|/det|>\n",
    "    <|ref|>image_caption<|/ref|><|det|>[[x1, y1, x2, y2]]<|/det|>\n",
    "    Caption text here...\n",
    "    \"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return {'text': '', 'figures': []}\n",
    "    \n",
    "    figures = []\n",
    "\n",
    "    # image_pattern = re.compile(\n",
    "    #     r\"\"\"\n",
    "    #     <\\|ref\\|>(?P<kind>image|table)<\\|/ref\\|>\\s*\n",
    "    #     <\\|det\\|>\\s*\\[\\[(?P<box>[^]]+)\\]\\]\\s*<\\|/det\\|>\\s*\n",
    "    #     <\\|ref\\|>(?P<cap_kind>image_caption|table_caption)<\\|/ref\\|>\\s*\n",
    "    #     <\\|det\\|>\\s*\\[\\[(?P<cap_box>[^]]+)\\]\\]\\s*<\\|/det\\|>\\s*\n",
    "    #     \"\"\",\n",
    "    #     re.IGNORECASE | re.DOTALL | re.VERBOSE\n",
    "    # )\n",
    "    # image_matches = list(re.finditer(image_pattern, text))\n",
    "\n",
    "    image_pattern = re.compile(\n",
    "        r\"\"\"\n",
    "        (?:\n",
    "            # 패턴 1: figure/table -> caption\n",
    "            <\\|ref\\|>(?P<kind1>image|table)<\\|/ref\\|>\\s*\n",
    "            <\\|det\\|>\\s*\\[\\[(?P<box1>[^]]+)\\]\\]\\s*<\\|/det\\|>\\s*\n",
    "            <\\|ref\\|>(?P<cap_kind1>image_caption|table_caption)<\\|/ref\\|>\\s*\n",
    "            <\\|det\\|>\\s*\\[\\[(?P<cap_box1>[^]]+)\\]\\]\\s*<\\|/det\\|>\n",
    "        |\n",
    "            # 패턴 2: caption -> figure/table\n",
    "            <\\|ref\\|>(?P<cap_kind2>image_caption|table_caption)<\\|/ref\\|>\\s*\n",
    "            <\\|det\\|>\\s*\\[\\[(?P<cap_box2>[^]]+)\\]\\]\\s*<\\|/det\\|>\\s*\n",
    "            (?:[^\\n<]+\\n)?                  # 선택적: caption 텍스트 한 줄\n",
    "            \\s*\\n                           # 빈 줄\n",
    "            <\\|ref\\|>(?P<kind2>image|table)<\\|/ref\\|>\\s*\n",
    "            <\\|det\\|>\\s*\\[\\[(?P<box2>[^]]+)\\]\\]\\s*<\\|/det\\|>\n",
    "        )\n",
    "        \"\"\",\n",
    "        re.IGNORECASE | re.DOTALL | re.VERBOSE\n",
    "    )\n",
    "    image_matches = list(re.finditer(image_pattern, text))\n",
    "\n",
    "    figures = []\n",
    "\n",
    "    for i, match in enumerate(image_matches, start=1):\n",
    "        # kind = match.group(\"kind\").lower() # image or table\n",
    "        # bbox = match.group(\"box\")\n",
    "        # caption_kind = match.group(\"cap_kind\").lower()\n",
    "        # caption_bbox = match.group(\"cap_box\")\n",
    "\n",
    "        # 어느 패턴에 매칭되었는지 확인\n",
    "        if match.group('kind1'):  # 패턴 1\n",
    "            kind = match.group('kind1').lower()\n",
    "            bbox = match.group('box1')\n",
    "            caption_kind = match.group('cap_kind1').lower()\n",
    "            caption_bbox = match.group('cap_box1')\n",
    "        else:  # 패턴 2\n",
    "            kind = match.group('kind2').lower()\n",
    "            bbox = match.group('box2')\n",
    "            caption_kind = match.group('cap_kind2').lower()\n",
    "            caption_bbox = match.group('cap_box2')\n",
    "    \n",
    "        if caption_kind.split('_')[1] != 'caption' or caption_kind.split('_')[0] != kind:\n",
    "            raise ValueError(\"Mismatch between figure/table and caption types.\")\n",
    "        \n",
    "        try:\n",
    "            # 좌표 파싱: \"506, 317, 914, 630\" -> [506, 317, 914, 630]\n",
    "            coords = [float(x.strip()) for x in bbox.split(',')]\n",
    "            caption_coords = [float(x.strip()) for x in caption_bbox.split(',')]\n",
    "            if len(coords) == 4 and len(caption_coords) == 4:\n",
    "                # 좌표가 1000 기준으로 normalize되어 있으면 0-1로 변환\n",
    "                if all(c >= 1 for c in coords):\n",
    "                    coords = [c / 1000.0 for c in coords]\n",
    "                if all(c >= 1 for c in caption_coords):\n",
    "                    caption_coords = [c / 1000.0 for c in caption_coords]\n",
    "\n",
    "                figure_dict = {\n",
    "                    \"type\": kind, \n",
    "                    \"number\": i,\n",
    "                    \"bbox\": coords,\n",
    "                    \"caption_bbox\": caption_coords\n",
    "                }\n",
    "                figures.append(figure_dict)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"좌표 파싱 오류: {e}\")\n",
    "            continue\n",
    "\n",
    "    return figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f2b5b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_caption(client, caption: str) -> str:\n",
    "    \"\"\"\n",
    "    Sends a message to the OpenAI Chat Completions API and returns the response.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = \"\"\"You are an expert in scientific document parsing.\n",
    "\n",
    "Your task is to extract structured information from figure, table, or equation captions.\n",
    "\n",
    "Each caption may begin with labels such as:\n",
    "\n",
    "- \"Figure 1\", \"Fig. 2\", \"FIGURE 3\", \"Fig. 1a\"\n",
    "- \"Table I\", \"TABLE 3\", \"Tbl. 2\"\n",
    "- \"Eq. (5)\", \"Equation 4\", \"Algorithm 2\", \"Listing 1\"\n",
    "\n",
    "Your goal is to **separate** the **type**, **number**, and **text** clearly.\n",
    "\n",
    "Always output **valid JSON** in the form:\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"type\": \"<one of: figure | table | equation | algorithm | listing | appendix | theorem>\",\n",
    "  \"number\": \"<string — the normalized number or ID (e.g., '1', 'IV', '3b', 'B', '(5)')>\",\n",
    "  \"text\": \"<the remaining caption text after the label>\"\n",
    "}}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Rules:\n",
    "\n",
    "1. **Type recognition**\n",
    "    - Case-insensitive.\n",
    "    - Map variations to canonical types:\n",
    "        - “Fig.”, “Figure”, “FIGURE” → `\"figure\"`\n",
    "        - “Table”, “TABLE”, “Tbl.” → `\"table\"`\n",
    "        - “Eq.”, “Equation”, “Eqs.” → `\"equation\"`\n",
    "        - “Algorithm”, “Alg.” → `\"algorithm\"`\n",
    "        - “Listing”, “Code”, “Example” → `\"listing\"`\n",
    "        - “Appendix” → `\"appendix\"`\n",
    "        - “Theorem”, “Lemma”, “Proposition” → `\"theorem\"`\n",
    "2. **Number normalization**\n",
    "    - Keep the numeric or letter identifier only (e.g., “1”, “1a”, “IV”, “B”, “(5)”).\n",
    "    - Preserve parentheses if they are part of equation format.\n",
    "3. **Text extraction**\n",
    "    - Remove the label and delimiter (like “:”, “.”, “–”, “—”) that follows it.\n",
    "    - Preserve all descriptive text exactly as written.\n",
    "4. **If no recognizable label appears**, output `\"type\": \"unknown\"`, `\"number\": \"\"`, and treat the full input as `\"text\"`.\n",
    "\n",
    "---\n",
    "\n",
    "### Examples:\n",
    "\n",
    "**Input:**\n",
    "\n",
    "`Fig. 1: A sample response generated by the conventional RAG pipeline (Vanilla RAG) and our proposed pipeline (Advanced RAG)...`\n",
    "\n",
    "**Output:*\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"type\": \"figure\",\n",
    "  \"number\": \"1\",\n",
    "  \"text\": \"A sample response generated by the conventional RAG pipeline (Vanilla RAG) and our proposed pipeline (Advanced RAG) for a given user query. Our proposed advanced RAG pipeline improves the response by retrieving an additional relevant text chunk for the LLM while eliminating an irrelevant one retrieved by the Vanilla RAG pipeline. The orange symbols indicate the English translation of the Bangla texts.\"\n",
    "}}\n",
    "```\n",
    "\n",
    "**Input:**\n",
    "\n",
    "`TABLE IV — Results of ablation experiments.`\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"type\": \"table\",\n",
    "  \"number\": \"4\",\n",
    "  \"text\": \"Results of ablation experiments.\"\n",
    "}}\n",
    "```\n",
    "\n",
    "**Input:**\n",
    "\n",
    "`Algorithm 3: Pseudo-code for iterative training loop.`\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"type\": \"algorithm\",\n",
    "  \"number\": \"3\",\n",
    "  \"text\": \"Pseudo-code for iterative training loop.\"\n",
    "}}\n",
    "```\n",
    "\n",
    "**Input:**\n",
    "\n",
    "`Eq. (5): The loss function is defined as follows.`\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"type\": \"equation\",\n",
    "  \"number\": \"5\",\n",
    "  \"text\": \"The loss function is defined as follows.\"\n",
    "}}\n",
    "```\n",
    "\n",
    "Return only a valid JSON object with exactly these three keys and no extra text.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",  # gpt-3.5-turbo\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are the best text parser for scientific documents.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"{prompt}\\nCaption: {caption}\"}\n",
    "            ]\n",
    "        )\n",
    "        result = response.choices[0].message.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"캡션 파싱 오류: {e}\"\n",
    "    return json.loads(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e9f51b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "처리 중: LegalRAG-A_Hybrid_RAG_System_for_Multilingual_Legal_Information_Retrieval\n",
      "================================================================================\n",
      "\n",
      "페이지 3/8: LegalRAG-A_Hybrid_RAG_System_for_Multilingual_Legal_Information_Retrieval_page_003.png\n",
      "images/LegalRAG-A_Hybrid_RAG_System_for_Multilingual_Legal_Information_Retrieval_page_003.png\n",
      "Inside locate_figures_with_grounding - Processing image: images/LegalRAG-A_Hybrid_RAG_System_for_Multilingual_Legal_Information_Retrieval_page_003.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "BASE:  torch.Size([1, 256, 1280])\n",
      "PATCHES:  torch.Size([6, 100, 1280])\n",
      "=====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OCR Result\n",
      "<|ref|>title<|/ref|><|det|>[[210, 63, 355, 75]]<|/det|>\n",
      "# III. METHODOLOGY\n",
      "\n",
      "<|ref|>title<|/ref|><|det|>[[77, 83, 245, 94]]<|/det|>\n",
      "# A. Problem Formulation\n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[77, 102, 491, 370]]<|/det|>\n",
      "Given a collection of \\(n\\)  government gazettes, we con-catenate them to form a single source document, denoted as \\(D\\) . This document is then divided into \\(m\\)  smaller text chunks, represented as \\(C=\\{C_{1},C_{2},\\ldots C_{m}\\}\\) , to facilitate efficient retrieval. Each chunk is processed through an em-bedding model, which generates corresponding embeddings \\(E=\\{E_{1},E_{2}\\ldots E_{m}\\}\\) . These embeddings are stored in a vector database to enable efficient similarity-based retrieval.During retrieval, relevant text chunks from C are retrieved by computing the similarity between the stored embeddings E and the embedding of the user query, denoted as \\(E_{Q}\\) . In the conventional vanilla RAG pipeline, the retrieved text chunks are directly passed to a generative LLM L, along with the query Q, to generate responses. However, in our proposed advanced RAG framework, the retrieved texts are first fed into a relevance check and query refinement LLM to ensure their relevance to the query Q. The refined and validated texts are then passed to the generative LLM L for response generation.\n",
      "\n",
      "<|ref|>title<|/ref|><|det|>[[77, 380, 135, 390]]<|/det|>\n",
      "# B. Data\n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[77, 399, 491, 741]]<|/det|>\n",
      "**Regulatory Document:** In this study, we utilize a collec-tion of official Bangladesh Police Gazettes published by the Bangladesh Government Press (BG Press). The Bangladesh Gazette is a weekly and regular publication by the Gov-ernment of Bangladesh, with occasional additional issues.The Bangladesh Police Gazettes provide legal and regulatory updates on police operations, protocols, and administrative directives. For our experiments, we use all 13 available gazettes from the BG Press archived published between 2016 and 2023. These documents are a mixture of English and Bangla, a low-resource language, with the ratio shown in Figure 2(B). The length of the gazettes varies, with the longest spanning 36 pages and the shortest consisting of a single page. The documents cover a wide range of topics, including updates to police regulations and descriptions of specialized police units such as the Anti-Terrorism Unit, Tourist Police,and River Police. Additionally, they contain administrative directives related to police training, investigation procedures,arrest protocols, and inter-agency coordination. The bilingual nature and complex structure of these gazettes necessitate the application of specialized NLP techniques to facilitate effective legal information retrieval. Table  presents key statistics for the regulatory documents used in this study.\n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[77, 744, 491, 833]]<|/det|>\n",
      "Data Preprocessing: The Bangladesh Police Gazettes are originally available in PDF format. However, standard PDF loaders do not perform well on Bangla texts, necessitating their conversion into a text-based format. To achieve this,Optical Character Recognition (OCR) techniques were applied 2. The preprocessing steps are outlined below:\n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[92, 837, 491, 880]]<|/det|>\n",
      "1) Each page of the PDF is first converted into an image using the pypdfium2 library and saved in JPEG format for efficient processing.\n",
      "\n",
      "<|ref|>table_caption<|/ref|><|det|>[[506, 58, 920, 84]]<|/det|>\n",
      "TABLE I: Key statistics from the employed Bangladesh Police Gazettes (2016-2023).\n",
      "\n",
      "<|ref|>table<|/ref|><|det|>[[560, 94, 866, 184]]<|/det|>\n",
      "\n",
      "<table><tr><td>Metric</td><td>Value</td></tr><tr><td>Total Gazettes</td><td>13</td></tr><tr><td>Total Pages</td><td>81</td></tr><tr><td>Minimum Pages in a Gazette</td><td>1</td></tr><tr><td>Maximum Pages in a Gazette</td><td>36</td></tr><tr><td>Average Pages per Gazette</td><td>6.23</td></tr></table>\n",
      "\n",
      "<|ref|>image<|/ref|><|det|>[[510, 207, 917, 355]]<|/det|>\n",
      "\n",
      "<|ref|>image_caption<|/ref|><|det|>[[506, 364, 920, 406]]<|/det|>\n",
      "<center>Fig. 2: (A) Distribution of question-answer pair domains in the curated evaluation dataset. (B) Language distribution in the bilingual regulatory document (Bangladesh Police Gazettes).</center>\n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[522, 437, 920, 552]]<|/det|>\n",
      "2) Tesseract OCR is configured to recognize both Bangla and English texts from the images. Several preprocessing techniques, including median filtering, contrast enhance-ment, and black & white conversion, are applied to enhance the image clarity and improve OCR accuracy.3) The extracted text from each page is then combined into a structured text document, making it suitable for further analysis.\n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[506, 560, 920, 799]]<|/det|>\n",
      "**Evaluation** **Dataset:** The preprocessed regulatory document texts are segmented and provided as input to OpenAI's GPT-4o 23, using tailored prompts to generate a series of question-answer pairs, both specifically in Bangla. Two of the authors serve as evaluators, cross-checking the generated pairs to eliminate inaccuracies. The evaluation set is carefully curated to ensure diversity, allowing for a rigorous assessment of the model's ability to effectively comprehend and process a low-resource language. The dataset includes questions span-ning multiple categories, such as factual inquiries, temporal changes, statistical reasoning, and variations in Bangla di-alects, as depicted in Figure 2(A). This diversity is crucial for testing the system's ability to handle various aspects of language comprehension and reasoning, from retrieving specific facts to interpreting context-dependent meanings in both formal and colloquial Bangla.\n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[506, 802, 920, 905]]<|/det|>\n",
      "Additionally, out-of-context questions are incorporated to assess the system's ability to filter relevant information and avoid distractions from unrelated content. The evaluation set also includes questions with spelling and grammatical errors to test the model's robustness in handling common linguistic inaccuracies, reflecting real-world challenges with noisy or complex text. By drawing questions from diverse contexts, the\n",
      "발견된 figure/table: 2개\n",
      "  - 1th table\n",
      "    - 크롭 저장 완료: extracted/LegalRAG-A_Hybrid_RAG_System_for_Multilingual_Legal_Information_Retrieval_page_003/table_001.png\n",
      "    - 크롭 저장 완료: extracted/LegalRAG-A_Hybrid_RAG_System_for_Multilingual_Legal_Information_Retrieval_page_003/table_caption_001.png\n",
      "=====================\n",
      "BASE:  torch.Size([1, 256, 1280])\n",
      "PATCHES:  torch.Size([9, 100, 1280])\n",
      "=====================\n",
      "    - caption 추출 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    - caption 파싱 완료: table I\n",
      "=====================\n",
      "BASE:  torch.Size([1, 256, 1280])\n",
      "PATCHES:  torch.Size([3, 100, 1280])\n",
      "=====================\n",
      "    - table 추출 완료\n",
      "  - 2th image\n",
      "    - 크롭 저장 완료: extracted/LegalRAG-A_Hybrid_RAG_System_for_Multilingual_Legal_Information_Retrieval_page_003/image_002.png\n",
      "    - 크롭 저장 완료: extracted/LegalRAG-A_Hybrid_RAG_System_for_Multilingual_Legal_Information_Retrieval_page_003/image_caption_002.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "BASE:  torch.Size([1, 256, 1280])\n",
      "PATCHES:  torch.Size([8, 100, 1280])\n",
      "=====================\n",
      "    - caption 추출 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    - caption 파싱 완료: figure 2\n",
      "=====================\n",
      "BASE:  torch.Size([1, 256, 1280])\n",
      "PATCHES:  torch.Size([2, 100, 1280])\n",
      "=====================\n",
      "    - figure description 생성 완료\n",
      "\n",
      "\n",
      "총 2개 figure/table 추출 완료\n",
      "메타데이터 저장: data/figures_data.json\n"
     ]
    }
   ],
   "source": [
    "# 모든 페이지에서 figure/table 추출\n",
    "all_figures_data = []\n",
    "\n",
    "for pdf_name, image_paths in all_page_images.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"처리 중: {pdf_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for page_idx, image_path in enumerate(image_paths, 1):\n",
    "        if page_idx != 3:\n",
    "            continue\n",
    "        print(f\"\\n페이지 {page_idx}/{len(image_paths)}: {image_path.name}\")\n",
    "        print(str(image_path))\n",
    "        \n",
    "        # Figure/Table 감지\n",
    "        result = locate_figures_with_grounding(tokenizer, model, str(image_path))\n",
    "        print()\n",
    "        print(f\"OCR Result\\n{result}\")\n",
    "\n",
    "        \n",
    "        figures = parse_grounding_result(result)\n",
    "        print(f\"발견된 figure/table: {len(figures)}개\")\n",
    "\n",
    "        extracted_page_dir = EXTRACTED_DIR / image_path.stem\n",
    "        if not extracted_page_dir.exists():\n",
    "            extracted_page_dir.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        for figure_dict in figures:\n",
    "            print(f\"  - {figure_dict['number']}th {figure_dict['type']}\")\n",
    "            \n",
    "            figure_crop_path = extracted_page_dir / f\"{figure_dict['type']}_{str(figure_dict['number']).zfill(3)}.png\"\n",
    "            caption_crop_path = extracted_page_dir / f\"{figure_dict['type']}_caption_{str(figure_dict['number']).zfill(3)}.png\"\n",
    "\n",
    "            crop_and_save_figure(image_path, figure_dict['bbox'], figure_crop_path)\n",
    "            crop_and_save_figure(image_path, figure_dict['caption_bbox'], caption_crop_path)\n",
    "\n",
    "            caption = free_ocr(tokenizer, model, caption_crop_path)\n",
    "            print(f\"    - caption 추출 완료\")\n",
    "            caption_dict = parse_caption(client, caption)\n",
    "            print(f\"    - caption 파싱 완료: {caption_dict['type']} {caption_dict['number']}\")\n",
    "\n",
    "            if figure_dict['type'] == 'table' and caption_dict['type'] == 'table':\n",
    "                figure_description = extract_table(tokenizer, model, figure_crop_path)\n",
    "                print(f\"    - table 추출 완료\")\n",
    "            else:\n",
    "                figure_description = generate_detailed_description(tokenizer, model, figure_crop_path)\n",
    "                print(f\"    - figure description 생성 완료\")\n",
    "\n",
    "            figure_data = {\n",
    "                'pdf_name': pdf_name,\n",
    "                'page': page_idx,\n",
    "                'type': caption_dict['type'],\n",
    "                'recognized_type': figure_dict['type'],\n",
    "                'number': caption_dict['number'],\n",
    "                'name': f\"{caption_dict['type'].capitalize()} {caption_dict['number']}\",\n",
    "                'caption': caption_dict['text'],\n",
    "                'description': figure_description,\n",
    "                'bbox': figure_dict['bbox'],\n",
    "                'caption_bbox': figure_dict['caption_bbox'],\n",
    "                'figure_path': str(figure_crop_path),\n",
    "                'caption_path': str(caption_crop_path),\n",
    "                'source_image': str(image_path)\n",
    "            }\n",
    "            \n",
    "            # 메타데이터 저장\n",
    "            all_figures_data.append(figure_data)\n",
    "\n",
    "# 메타데이터 저장\n",
    "figure_data_path = DATA_DIR / \"figures_data.json\"\n",
    "with open(figure_data_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_figures_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n\\n총 {len(all_figures_data)}개 figure/table 추출 완료\")\n",
    "print(f\"메타데이터 저장: {figure_data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337fe330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ac9736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e09f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e8ef19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc69dbd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a37ac5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eee4dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c5ecd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2a95df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faff8d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a84d10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf2ba6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcb49a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a295fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
